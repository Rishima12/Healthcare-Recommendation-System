{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKCTciOumL2k",
        "outputId": "915ea901-463a-49c3-bf9d-d6bcc7b8aa41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading and Preprocessing data**"
      ],
      "metadata": {
        "id": "6uPDG98gM-kh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def load_and_preprocess_data(file_path, dataset_name):\n",
        "    # Loading the data\n",
        "    data = pd.read_csv(file_path,encoding='latin1')\n",
        "\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "# Loading and preprocessing each dataset\n",
        "\n",
        "demographic_data = load_and_preprocess_data(\n",
        "    '/content/drive/MyDrive/demographic.csv',\n",
        "    dataset_name='demographic'\n",
        ")\n",
        "\n",
        "diet_data = load_and_preprocess_data(\n",
        "    '/content/drive/MyDrive/diet.csv',\n",
        "    dataset_name='diet'\n",
        ")\n",
        "\n",
        "examination_data = load_and_preprocess_data(\n",
        "    '/content/drive/MyDrive/examination.csv',\n",
        "    dataset_name='examination'\n",
        ")\n",
        "\n",
        "labs_data = load_and_preprocess_data(\n",
        "    '/content/drive/MyDrive/labs.csv',\n",
        "    dataset_name='labs'\n",
        ")\n",
        "\n",
        "medications_data = load_and_preprocess_data(\n",
        "    '/content/drive/MyDrive/medications - medications.csv',\n",
        "    dataset_name='medications'\n",
        ")\n",
        "labeled_data=load_and_preprocess_data(\n",
        "    '/content/drive/MyDrive/Labeled_data - Sheet1 (1).csv',\n",
        "    dataset_name='labeled_data'\n",
        ")\n",
        "demographic_data = demographic_data.iloc[:, :3]\n",
        "diet_data = diet_data.iloc[:, :3]\n",
        "examination_data = examination_data.iloc[:, :3]\n",
        "labs_data = labs_data.iloc[:, :3]\n",
        "medications_data = medications_data.iloc[:, :3]\n",
        "\n",
        "# Merging the datasets based on common column\n",
        "data = pd.merge(demographic_data, diet_data,how='left')\n",
        "data = pd.merge(data, examination_data,how='left')\n",
        "data = pd.merge(data, labs_data,how='left')\n",
        "data = pd.merge(data, medications_data,how='left')\n",
        "data = pd.merge(data, labeled_data,how='left')\n",
        "data=data.dropna()\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "435u240gvMst",
        "outputId": "81627ad0-a289-4d9e-9e05-acca78603a59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        SEQN  SDDSRVYR  RIDSTATR        WTDRD1        WTDR2D  PEASCST1  \\\n",
            "0      73557         8         2  16888.327864  12930.890649       1.0   \n",
            "1      73557         8         2  16888.327864  12930.890649       1.0   \n",
            "2      73558         8         2  17932.143865  12684.148869       1.0   \n",
            "3      73558         8         2  17932.143865  12684.148869       1.0   \n",
            "4      73558         8         2  17932.143865  12684.148869       1.0   \n",
            "...      ...       ...       ...           ...           ...       ...   \n",
            "20182  83724         8         2  32115.849467  46169.498764       1.0   \n",
            "20183  83724         8         2  32115.849467  46169.498764       1.0   \n",
            "20184  83724         8         2  32115.849467  46169.498764       1.0   \n",
            "20185  83724         8         2  32115.849467  46169.498764       1.0   \n",
            "20186  83724         8         2  32115.849467  46169.498764       1.0   \n",
            "\n",
            "       PEASCTM1  URXUMA  URXUMS  RXDUSE  RXDCOUNT  SCORE  \n",
            "0         620.0     4.3     4.3       1       2.0    4.0  \n",
            "1         620.0     4.3     4.3       1       2.0    4.0  \n",
            "2         766.0   153.0   153.0       1       4.0    1.0  \n",
            "3         766.0   153.0   153.0       1       4.0    1.0  \n",
            "4         766.0   153.0   153.0       1       4.0    1.0  \n",
            "...         ...     ...     ...     ...       ...    ...  \n",
            "20182    1419.0     6.4     6.4       1       5.0    1.0  \n",
            "20183    1419.0     6.4     6.4       1       5.0    1.0  \n",
            "20184    1419.0     6.4     6.4       1       5.0    1.0  \n",
            "20185    1419.0     6.4     6.4       1       5.0    1.0  \n",
            "20186    1419.0     6.4     6.4       1       5.0    1.0  \n",
            "\n",
            "[11253 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction using CNN model**"
      ],
      "metadata": {
        "id": "uVxI3FbaNd8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "# Defining input features\n",
        "input_features = ['SDDSRVYR', 'RIDSTATR', 'WTDRD1', 'WTDR2D',\n",
        "                  'PEASCST1', 'PEASCTM1', 'URXUMA', 'URXUMS',\n",
        "                  'RXDUSE', 'RXDCOUNT']\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Preparing input data and labels\n",
        "train_features = train_data[input_features]\n",
        "train_labels = train_data['SCORE']\n",
        "\n",
        "test_features = test_data[input_features]\n",
        "test_labels = test_data['SCORE']\n",
        "\n",
        "# Converting NumPy arrays to tensors\n",
        "train_features = tf.convert_to_tensor(train_features.values, dtype=tf.float32)\n",
        "train_labels = tf.convert_to_tensor(train_labels.values, dtype=tf.float32)\n",
        "\n",
        "test_features = tf.convert_to_tensor(test_features.values, dtype=tf.float32)\n",
        "test_labels = tf.convert_to_tensor(test_labels.values, dtype=tf.float32)\n",
        "\n",
        "# Defining the 1D CNN model\n",
        "input_layer = Input(shape=(len(input_features), 1))  # Input shape for 1D CNN\n",
        "conv1d_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
        "maxpooling1d_layer = MaxPooling1D(pool_size=2)(conv1d_layer)\n",
        "flatten_layer = Flatten()(maxpooling1d_layer)\n",
        "dense_layer_1 = Dense(64, activation='relu')(flatten_layer)\n",
        "output_layer = Dense(1, activation='linear')(dense_layer_1)  # Linear activation for regression task\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Training the model\n",
        "model.fit(train_features, train_labels, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluating the model on test data\n",
        "loss = model.evaluate(test_features, test_labels)\n",
        "print(f\"Test Loss (MSE): {loss}\")\n",
        "\n",
        "# Making predictions on new data\n",
        "new_data = pd.DataFrame({\n",
        "    'SDDSRVYR': [8],\n",
        "    'RIDSTATR': [2],\n",
        "    'WTDRD1': [16888.327864],\n",
        "    'WTDR2D': [12930.890649],\n",
        "    'PEASCST1': [1],\n",
        "    'PEASCTM1': [620],\n",
        "    'URXUMA': [4.3],\n",
        "    'URXUMS': [4.3],\n",
        "    'RXDUSE': [1],\n",
        "    'RXDCOUNT': [2]\n",
        "})\n",
        "\n",
        "# Converting the new data to a tensor\n",
        "new_data = tf.convert_to_tensor(new_data.values, dtype=tf.float32)\n",
        "\n",
        "# Reshaping the input data to match the 1D CNN input shape\n",
        "new_data = tf.reshape(new_data, shape=(1, len(input_features), 1))\n",
        "\n",
        "predicted_scores = model.predict(new_data)\n",
        "print(f\"Predicted Healthcare Score: {predicted_scores[0][0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfAZisgGvabJ",
        "outputId": "fd80da25-79fa-4866-ba6e-dc3af96bfb66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "282/282 [==============================] - 2s 3ms/step - loss: 445283.0000\n",
            "Epoch 2/10\n",
            "282/282 [==============================] - 1s 3ms/step - loss: 52643.1914\n",
            "Epoch 3/10\n",
            "282/282 [==============================] - 1s 3ms/step - loss: 323711.7500\n",
            "Epoch 4/10\n",
            "282/282 [==============================] - 1s 3ms/step - loss: 72850.2891\n",
            "Epoch 5/10\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 1994.8624\n",
            "Epoch 6/10\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 3417.0308\n",
            "Epoch 7/10\n",
            "282/282 [==============================] - 1s 4ms/step - loss: 112174.4141\n",
            "Epoch 8/10\n",
            "282/282 [==============================] - 1s 5ms/step - loss: 1916.6345\n",
            "Epoch 9/10\n",
            "282/282 [==============================] - 1s 2ms/step - loss: 1524.0609\n",
            "Epoch 10/10\n",
            "282/282 [==============================] - 1s 2ms/step - loss: 158993.5156\n",
            "71/71 [==============================] - 0s 2ms/step - loss: 15803.0791\n",
            "Test Loss (MSE): 15803.0791015625\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "Predicted Healthcare Score: 21.71817398071289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction using feed-forward neural network**"
      ],
      "metadata": {
        "id": "ZS40ibL3NsNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Defining input features\n",
        "input_features = ['SDDSRVYR', 'RIDSTATR', 'WTDRD1', 'WTDR2D',\n",
        "                  'PEASCST1', 'PEASCTM1', 'URXUMA', 'URXUMS',\n",
        "                  'RXDUSE', 'RXDCOUNT']\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.33, random_state=42)\n",
        "\n",
        "# Preparing input data\n",
        "train_features = train_data[input_features]\n",
        "train_labels = train_data['SCORE']\n",
        "\n",
        "train_features = tf.convert_to_tensor(train_features.values, dtype=tf.float32)\n",
        "train_labels = tf.convert_to_tensor(train_labels.values, dtype=tf.float32)\n",
        "\n",
        "# Defining the neural network model\n",
        "input_layer = Input(shape=(len(input_features),))\n",
        "dense_layer_1 = Dense(64, activation='relu')(input_layer)\n",
        "dense_layer_2 = Dense(32, activation='linear')(dense_layer_1)\n",
        "output_layer = Dense(1, activation='linear')(dense_layer_2)  # Linear activation for regression task\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(optimizer='adam', loss='mean_absolute_error')\n",
        "\n",
        "# Training the model\n",
        "model.fit(train_features, train_labels, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluating the model on test data\n",
        "test_features = test_data[input_features]\n",
        "test_labels = test_data['SCORE']\n",
        "\n",
        "test_features = tf.convert_to_tensor(test_features.values, dtype=tf.float32)\n",
        "test_labels = tf.convert_to_tensor(test_labels.values, dtype=tf.float32)\n",
        "\n",
        "loss = model.evaluate(test_features, test_labels)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "\n",
        "# Making predictions on new data\n",
        "new_data = pd.DataFrame({\n",
        "    'SDDSRVYR': [8],\n",
        "    'RIDSTATR': [2],\n",
        "    'WTDRD1': [16888.327864],\n",
        "    'WTDR2D': [12930.890649],\n",
        "    'PEASCST1': [1],\n",
        "    'PEASCTM1': [620],\n",
        "    'URXUMA': [4.3],\n",
        "    'URXUMS': [4.3],\n",
        "    'RXDUSE': [1],\n",
        "    'RXDCOUNT': [2]\n",
        "})\n",
        "new_data = tf.convert_to_tensor(new_data.values, dtype=tf.float32)\n",
        "\n",
        "predicted_scores = model.predict(new_data)\n",
        "print(f\"Predicted Healthcare Score: {predicted_scores[0][0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5ZtX9W1FiCC",
        "outputId": "2225dcd0-6ba9-40eb-9197-025bf401db8b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "236/236 [==============================] - 2s 5ms/step - loss: 1371.5807\n",
            "Epoch 2/10\n",
            "236/236 [==============================] - 1s 4ms/step - loss: 212.0736\n",
            "Epoch 3/10\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 227.0077\n",
            "Epoch 4/10\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 232.8371\n",
            "Epoch 5/10\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 196.0447\n",
            "Epoch 6/10\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 249.7493\n",
            "Epoch 7/10\n",
            "236/236 [==============================] - 1s 2ms/step - loss: 234.2869\n",
            "Epoch 8/10\n",
            "236/236 [==============================] - 1s 3ms/step - loss: 200.9560\n",
            "Epoch 9/10\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 126.5842\n",
            "Epoch 10/10\n",
            "236/236 [==============================] - 0s 2ms/step - loss: 205.3247\n",
            "117/117 [==============================] - 0s 2ms/step - loss: 65.9123\n",
            "Test Loss: 65.91228485107422\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7a3fec582f80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 72ms/step\n",
            "Predicted Healthcare Score: 19.607385635375977\n"
          ]
        }
      ]
    }
  ]
}